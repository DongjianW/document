#Install Hadoop with Pseudo-distributed
#ENV:
CentOS 7.0

#Reference
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

#Install JDK
1. yum list -y jdk 
1. yum install java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64
2. cat "export JAVA_HOME=/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64" >>/etc/profile
3. source /etc/profile
4. edit etc/hadoop/hadoop-env.sh 
JAVA_HOME=/lib/jvm/java-1.8.0-openjdk-1.8.0.111-2.b15.el7_3.x86_64

#Install Hadoop 
1. wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
2. tar zxvf hadoop-2.7.3.tar.gz
3. cp -r hadoop-2.7.3 /usr/local

#Config Hadoop
1. cd /usr/local/hadoop-2.7.3/etc/hadoop
2. edit etc/hadoop/core-site.xml:
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
3. edit etc/hadoop/hdfs-site.xml:
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

#Config passphraseless ssh 
1. Now check that you can ssh to the localhost without a passphrase:

  $ ssh localhost
2. If you cannot ssh to localhost without a passphrase, execute the following commands:

  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys

#Execution
The following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, see YARN on Single Node.
Format the filesystem:

  $ bin/hdfs namenode -format
Start NameNode daemon and DataNode daemon:

  $ sbin/start-dfs.sh
The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs).

Browse the web interface for the NameNode; by default it is available at:

NameNode - http://localhost:50070/
Make the HDFS directories required to execute MapReduce jobs:

  $ bin/hdfs dfs -mkdir /user
  $ bin/hdfs dfs -mkdir /user/root
  $ bin/hdfs dfs -mkdir /user/root/input
Copy the input files into the distributed filesystem:

  $ bin/hdfs dfs -put etc/hadoop/*.xml input
Run some of the examples provided:

  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'
Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:

  $ bin/hdfs dfs -get output output
  $ cat output/*
or

View the output files on the distributed filesystem:

  $ bin/hdfs dfs -cat output/*
When youâ€™re done, stop the daemons with:

  $ sbin/stop-dfs.sh
